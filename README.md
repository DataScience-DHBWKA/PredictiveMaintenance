ğŸ“Š Predictive Maintenance with Machine Learning

ğŸ¯ Project Overview

This project explores the use of machine learning (ML) techniques to predict machine failures based on sensor data, contributing significantly to reducing unplanned downtime.

ğŸ“š Data Understanding

ğŸ” Dataset

Source: Synthetic sensor data from milling machines (UCI ML Repository).

Features:

Numerical: Air Temperature, Process Temperature, Rotational Speed, Torque, Tool Wear

Categorical: Machine Type (L/M/H)

Target Variables: Types of failures (TWF, HDF, PWF, OSF, RNF)

ğŸ“ˆ Exploratory Data Analysis (EDA)

Conducted statistical analyses and created visualizations:

ğŸ“Œ Class imbalance identified (3.39% failure rate)

ğŸ“Œ Feature insights: High correlation between rotational speed & torque (-0.88)

ğŸ“Œ Significant patterns:

HDF occurs at higher temperatures

PWF linked to higher rotational speeds

TWF and OSF occur with high tool wear

ğŸ› ï¸ Data Preparation

âš™ï¸ Steps Undertaken

Data Import & Type Adjustment:

Corrected data types manually during import (e.g., numerical, categorical, binary).

Applied one-hot encoding to categorical features.

Data Splitting & Oversampling:

Split into Training/Test sets (80:20).

Applied SMOTETomek to address class imbalance in training set.

Data Scaling:

StandardScaler used for numerical variables.

ğŸ’¾ Resulting Datasets

Training data: dataset_train_resampled.csv

Testing data: dataset_test.csv

ğŸ¤– Modeling

ğŸ¯ Problem Definition

Multi-class classification (6 categories: TWF, HDF, PWF, OSF, RNF, no_failure)

ğŸ§© Models Implemented

Logistic Regression

Random Forest (with hyperparameter tuning)

Decision Trees (standard & pruned)

Support Vector Machines (SVM)

K-Nearest Neighbors (standard & optimized)

âš™ï¸ Hyperparameter Optimization

Grid Search with 5-fold cross-validation for Random Forest and KNN

Cost-Complexity-Pruning for Decision Trees

ğŸ§ª Evaluation

ğŸ“ Metrics Used

Accuracy, Precision, Recall, and F1-Score

ğŸ¥‡ Best Performing Model

ğŸŒ² Random Forest with Hyperparameter Tuning:

Training F1-Score: 1.000

Testing F1-Score: 0.9681

ğŸ“Š Scatterplot Analysis

Evaluated consistency between training and test metrics to identify overfitting.

ğŸ“‰ Bias-Variance Tradeoff

Analyzed flexibility parameters influencing model performance.

ğŸ—£ï¸ Discussion

Confirmed oversampling improved model generalization without data distortion.

Final evaluation conducted exclusively on original (non-synthetic) test data.

ğŸš€ Conclusions

ML effectively predicts machine failures.

Random Forest with hyperparameter tuning is highly reliable and robust.

Future work: Validation with real industry data and real-time system integration.

ğŸ“¦ Tools & Resources

Python, pandas, imbalanced-learn, scikit-learn

Visualization: Matplotlib, seaborn

ğŸ« Authors

Daniel Weissenberger

Eduardo Stein MÃ¶ssner

Jonas Sigmund

ğŸ‘©â€ğŸ« Supervisor

Prof. Dr. Jennifer Schoch

ğŸ“… Date

25.03.2025

ğŸŒŸ Thanks for exploring our Predictive Maintenance ML Project! ğŸŒŸ

