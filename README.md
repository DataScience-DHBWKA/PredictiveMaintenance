# ðŸ› ï¸ Predictive Maintenance with Machine Learning

This repository presents a comprehensive machine learning project aimed at identifying machine failures through sensor data analysis. The project follows the structured CRISP-MLQ methodology, emphasizing Data Understanding, Data Preparation, and Modeling with rigorous Evaluation.

---

## ðŸ“Š Data Understanding

> **ðŸ”Ž Objective:** Exploratory Data Analysis (EDA) to understand feature distributions, detect patterns, and identify correlations.

### ðŸ“Œ Dataset Overview

The dataset used contains sensor readings from CNC milling machines, featuring numeric and categorical attributes:

| Feature | Description | Type |
|---------|-------------|------|
| air_temperature | Air temperature (in Kelvin) | Numeric |
| process_temperature | Process temperature (in Kelvin) | Numeric |
| rotational_speed | Rotational speed (in rpm) | Numeric |
| torque | Torque (in Nm) | Numeric |
| tool_wear | Tool wear duration (in minutes) | Numeric |
| machine_failure | Machine failure status (binary) | Categorical |

### ðŸ“Œ Key Insights from EDA
- ðŸ”¥ **Heat Dissipation Failures** correlate strongly with high temperatures.
- âš™ï¸ **Power Failures** primarily occur at higher rotational speeds.
- ðŸ”§ **Tool Wear and Overstrain Failures** appear significantly with higher tool wear durations.

### ðŸ“Œ Correlation Highlights
- Strong negative correlation (Ï = -0.88) between torque and rotational speed.
- Positive correlation between air and process temperature.

> **ðŸ’¡ Tip:** Always visualize feature distributions and class imbalance early to inform data preparation strategies.

---

## ðŸ§¹ Data Preparation

> **ðŸ”„ Objective:** Transform raw data into a structured, balanced format suitable for modeling.

### ðŸ“Œ Steps Taken

1. **Data Import & Type Adjustment**
```python
import pandas as pd

# Explicitly set correct data types for optimization
 df = pd.read_csv("data.csv", dtype={
   'air_temperature': 'float32',
   'machine_failure': 'bool',
   'type': 'category',
})
```

2. **Feature Engineering**
- Categorical variable encoding via One-Hot-Encoding.
- Created consolidated target variable `label` combining all failure types.

3. **Addressing Class Imbalance**
- Applied SMOTETomek Oversampling on training data only to balance class distributions:
```python
from imblearn.combine import SMOTETomek

smote_tomek = SMOTETomek(random_state=42)
X_train_res, y_train_res = smote_tomek.fit_resample(X_train, y_train)
```

4. **Standardization**
- Numeric features standardized using `StandardScaler`:
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_res_scaled = scaler.fit_transform(X_train_res)
X_test_scaled = scaler.transform(X_test)
```

> **âš ï¸ Important:** Do not oversample test data to maintain evaluation validity.

---

## ðŸ¤– Modeling & ðŸ“ Evaluation

> **ðŸŽ¯ Objective:** Develop and compare machine learning models to accurately classify machine failures.

### ðŸ“Œ Problem Formulation
- Multi-class classification problem with 6 distinct classes: TWF, HDF, PWF, OSF, RNF, and No Failure.

### ðŸ“Œ Models Implemented
- Logistic Regression
- Random Forest (Best Performer ðŸŽ‰)
- Decision Tree (with and without pruning)
- Support Vector Machine (SVM)
- K-Nearest Neighbors (optimized k)

### ðŸ“Œ Hyperparameter Tuning
- Utilized Grid Search with 5-fold cross-validation:
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None]
}
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, scoring='f1_macro', cv=5)
grid_search.fit(X_train_res_scaled, y_train_res)
```

### ðŸ“Œ Evaluation Metrics
| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| **Random Forest (Tuned)** | **96.35%** | **97.34%** | **96.35%** | **96.81%** |
| Decision Tree (Pruned) | 94.95% | 97.52% | 94.95% | 96.18% |
| SVM | 85.35% | 97.17% | 85.35% | 90.52% |

- Random Forest showed the best balance between Bias and Variance.

### ðŸ“Œ Bias-Variance Analysis
- Models with tuned hyperparameters effectively reduced both Bias and Variance, enhancing generalization.

### ðŸ“Œ Insights from Oversampling
- Boxplot analysis confirmed oversampling did not distort data distributions, ensuring valid evaluation.

> **ðŸš€ Recommendation:** Random Forest (Hyperparameter-Tuned) is recommended due to high accuracy, excellent generalization, and robustness.

---

## ðŸ“‚ Repository Structure
```bash
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset_train_resampled.csv
â”‚   â””â”€â”€ dataset_test.csv
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ DataUnderstanding.ipynb
â”‚   â”œâ”€â”€ DataPreparation.ipynb
â”‚   â””â”€â”€ ModelBuilding.ipynb
â””â”€â”€ README.md
```

---

âœ¨ **Happy Modeling!** âœ¨
